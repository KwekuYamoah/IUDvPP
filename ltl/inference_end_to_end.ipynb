{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydub in /Users/kayems/.pyenv/versions/3.11.8/envs/ltlenv/lib/python3.11/site-packages (0.25.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# adding the directory containing asr/asr_pipeline.py to the Python path\n",
    "sys.path.append(os.path.abspath(\"../asr\"))\n",
    "#add prosody path to the python path\n",
    "sys.path.append(os.path.abspath(\"../prosody/utils\"))\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../prosody/training_scripts\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence, detect_silence\n",
    "from asr_pipeline import asr_infer_pipeline\n",
    "from prosody_feature_extraction import extract_prosody_features\n",
    "from prosody_bilstm_features_only import Encoder, Decoder, Seq2Seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASR\n",
    "Section to load audio from disk and get transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_wav(input_file: str, output_file: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts an audio file to .wav format.\n",
    "\n",
    "    Args:\n",
    "        input_file (str): The path to the input audio file.\n",
    "        output_file (str): The path to the output .wav file.\n",
    "\n",
    "    Returns:\n",
    "        str: The path to the converted .wav file.\n",
    "    \"\"\"\n",
    "    # Load the audio file\n",
    "    audio = AudioSegment.from_file(input_file)\n",
    "\n",
    "    # Export as .wav\n",
    "    audio.export(output_file, format=\"wav\")\n",
    "\n",
    "    return output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_silence_from_audio(audio_path, output_path, silence_thresh=-40, min_silence_len=10, keep_silence=0):\n",
    "    # Load the audio file\n",
    "    audio = AudioSegment.from_file(audio_path)\n",
    "\n",
    "    # Split audio on silence\n",
    "    chunks = split_on_silence(\n",
    "        audio,\n",
    "        min_silence_len=min_silence_len,\n",
    "        silence_thresh=silence_thresh,\n",
    "        keep_silence=keep_silence\n",
    "    )\n",
    "\n",
    "    # Concatenate the chunks back together\n",
    "    stripped_audio = AudioSegment.empty()\n",
    "    for chunk in chunks:\n",
    "        stripped_audio += chunk\n",
    "\n",
    "    # Export the new audio file without silence\n",
    "    stripped_audio.export(output_path, format=\"wav\")\n",
    "    print(f\"Saved stripped audio: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covert audio file to wav format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create TextGrid File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_aeneas_dynamic(audio_file_path: str, text_transcript: str, temp_dir: str) -> str:\n",
    "    \"\"\"\n",
    "    Call Aeneas to generate a TextGrid file for the given audio file and transcript.\n",
    "\n",
    "    Args:\n",
    "        audio_file_path (str): The path to the audio file.\n",
    "        text_transcript (str): The transcription text.\n",
    "        temp_dir (str): Directory for storing temporary files.\n",
    "\n",
    "    Returns:\n",
    "        str: The relative path to the generated TextGrid file.\n",
    "\n",
    "    Raises:\n",
    "        subprocess.CalledProcessError: If the command to run Aeneas fails.\n",
    "    \"\"\"\n",
    "    # Define directories based on the current working directory\n",
    "    current_dir = os.getcwd()\n",
    "    PROJECT_DIR = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "    AENEAS_DIR = os.path.join(PROJECT_DIR, 'aeneas')\n",
    "    TEMP_TEXT_FILE_PATH = os.path.join(temp_dir, 'temp_transcription.txt')\n",
    "    TEXT_GRID_DIR = os.path.join(temp_dir, 'text_grid_files')  # Create a text_grid_files folder in the temp directory\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    if not os.path.exists(TEXT_GRID_DIR):\n",
    "        os.makedirs(TEXT_GRID_DIR)\n",
    "        print(f\"Created directory {TEXT_GRID_DIR}\")\n",
    "\n",
    "    # Write the transcription text to the temp file\n",
    "    with open(TEMP_TEXT_FILE_PATH, 'w', encoding='utf-8') as temp_text_file:\n",
    "        for word in text_transcript.split():\n",
    "            temp_text_file.write(word + '\\n')\n",
    "\n",
    "    # Define the output file path\n",
    "    output_file_path = os.path.join(TEXT_GRID_DIR, f\"{os.path.splitext(os.path.basename(audio_file_path))[0]}.TextGrid\")\n",
    "\n",
    "    try:\n",
    "        # Change to the aeneas directory\n",
    "        os.chdir(AENEAS_DIR)\n",
    "\n",
    "        # Define the command to run aeneas\n",
    "        command = [\n",
    "            'python3', '-m', 'aeneas.tools.execute_task',\n",
    "            audio_file_path,\n",
    "            TEMP_TEXT_FILE_PATH,\n",
    "            'task_language=eng|is_text_type=plain|os_task_file_format=aud',\n",
    "            output_file_path\n",
    "        ]\n",
    "\n",
    "        # Run the command\n",
    "        subprocess.run(command, check=True)\n",
    "        print(f\"Generated TextGrid for {audio_file_path} to {output_file_path}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Failed to generate TextGrid for {audio_file_path}: {e}\")\n",
    "    finally:\n",
    "        # Return to the original directory\n",
    "        os.chdir(current_dir)\n",
    "        # Clean up the temporary text file\n",
    "        if os.path.exists(TEMP_TEXT_FILE_PATH):\n",
    "            os.remove(TEMP_TEXT_FILE_PATH)\n",
    "\n",
    "    return os.path.relpath(output_file_path, start=current_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Audio Slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_audio_slices(audio_path, textgrid_path, output_folder):\n",
    "    # Load the audio file\n",
    "    audio = AudioSegment.from_file(audio_path)\n",
    "    print(audio)\n",
    "\n",
    "    # Load the TextGrid file and read its contents\n",
    "    with open(textgrid_path, 'r') as f:\n",
    "        file_contents = f.readlines()\n",
    "\n",
    "    # Iterate through each line in the TextGrid file\n",
    "    for index, line in enumerate(file_contents):\n",
    "        # Split the line into start time, end time, and word\n",
    "        start_time_str, end_time_str, word = line.strip().split('\\t')\n",
    "        start_time = float(start_time_str)\n",
    "        end_time = float(end_time_str)\n",
    "\n",
    "        # Skip empty words\n",
    "        if not word:\n",
    "            continue\n",
    "\n",
    "        # Create a buffer window around the word, except for the first word\n",
    "        if index != 0:\n",
    "            start_time = max(0, start_time - 0.010)  # Subtract 5 milliseconds\n",
    "        end_time = end_time + 0.035  # Add 35 milliseconds\n",
    "\n",
    "        # Create a slice of the original audio\n",
    "        start_ms = int(start_time * 1000)  # Convert to milliseconds\n",
    "        end_ms = int(end_time * 1000)  # Convert to milliseconds\n",
    "        audio_slice = audio[start_ms:end_ms]\n",
    "\n",
    "        # Construct the filename\n",
    "        filename = f\"{os.path.basename(audio_path).split('.')[0]}_{word}_{index}_{start_time:.3f}_{end_time:.3f}.wav\"\n",
    "        output_path = os.path.join(output_folder, filename)\n",
    "\n",
    "        # Export the slice\n",
    "        audio_slice.export(output_path, format=\"wav\")\n",
    "        print(f\"Saved: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Json Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features_json(input_folder, output_json_path):\n",
    "    \"\"\"\n",
    "    Processes audio slices to extract prosodic features and save them in a JSON file.\n",
    "\n",
    "    Args:\n",
    "        input_folder (str): Folder containing sliced audio files.\n",
    "        output_json_path (str): Path to save the JSON file.\n",
    "    \"\"\"\n",
    "    # Create a dictionary to store the results\n",
    "    results = {}\n",
    "\n",
    "    # Iterate through each file in the input folder\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(\".wav\"):\n",
    "            # Construct the full file path\n",
    "            audio_path = os.path.join(input_folder, filename)\n",
    "\n",
    "            # Extract the root name, word, and position from the filename\n",
    "            parts = filename.split('_')\n",
    "            #remove the first index in parts\n",
    "            parts.pop(0)\n",
    "\n",
    "            root_name = parts[0]\n",
    "            word = parts[1]\n",
    "            position = int(parts[2])\n",
    "            start_time = float(parts[3])\n",
    "            end_time = float(parts[4].replace('.wav', ''))\n",
    "\n",
    "            # Extract prosodic features\n",
    "            features = extract_prosody_features(audio_path)\n",
    "            features[torch.isnan(features)] = 0\n",
    "\n",
    "            # Convert the tensor to a list\n",
    "            features_list = features.tolist()\n",
    "\n",
    "            # get current directory\n",
    "            current_dir = os.getcwd()\n",
    "\n",
    "            # Prepare data structure for the file\n",
    "            if root_name not in results:\n",
    "                results[root_name] = {\n",
    "                    \"filepath\": os.path.join(current_dir, root_name + \".wav\"),\n",
    "                    \"words\": [],\n",
    "                    \"positions\": [],\n",
    "                    \"features\": []\n",
    "                }\n",
    "\n",
    "            # Append the word, position, and features to the respective lists\n",
    "            results[root_name][\"words\"].append((position, word))\n",
    "            results[root_name][\"positions\"].append((position, position))\n",
    "            results[root_name][\"features\"].append((position, features_list))\n",
    "\n",
    "    # Sort the words, positions, and features by their positions\n",
    "    for root_name, data in results.items():\n",
    "        data[\"words\"] = [word for position, word in sorted(data[\"words\"])]\n",
    "        data[\"positions\"] = [position for position, _ in sorted(data[\"positions\"])]\n",
    "        data[\"features\"] = [features for position, features in sorted(data[\"features\"])]\n",
    "\n",
    "    # Write the results to a JSON file\n",
    "    with open(output_json_path, 'w') as json_file:\n",
    "        json.dump(results, json_file, indent=4)\n",
    "    print(f\"Saved results to {output_json_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer with Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path, feature_dim, hidden_dim=128, output_dim=4, num_layers=2, dropout=0.418213662555253, num_attention_layers=2, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Load the trained model from a specified file.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): Path to the model file.\n",
    "        feature_dim (int): Dimension of the input features.\n",
    "        hidden_dim (int): Dimension of the hidden state in LSTM.\n",
    "        output_dim (int): Dimension of the output layer.\n",
    "        num_layers (int): Number of LSTM layers.\n",
    "        dropout (float): Dropout rate.\n",
    "        num_attention_layers (int): Number of attention layers.\n",
    "        device (torch.device): Device to load the model onto (CPU or GPU).\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: Loaded Seq2Seq model.\n",
    "    \"\"\"\n",
    "    encoder = Encoder(feature_dim, hidden_dim, num_layers, dropout).to(device)\n",
    "    decoder = Decoder(hidden_dim, output_dim, num_layers, dropout, num_attention_layers).to(device)\n",
    "    model = Seq2Seq(encoder, decoder).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_labels(json_path, model, device, num_classes=2):\n",
    "    \"\"\"\n",
    "    Perform inference on a JSON file to predict labels for words.\n",
    "\n",
    "    Args:\n",
    "        json_path (str): Path to the input JSON file.\n",
    "        model (nn.Module): Trained model for prediction.\n",
    "        device (torch.device): Device to perform inference on (CPU or GPU).\n",
    "        num_classes (int): Number of classes in the classification task.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with word predictions.\n",
    "    \"\"\"\n",
    "    # Load data from JSON\n",
    "    with open(json_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    results = {}\n",
    "    for key, entry in data.items():\n",
    "        words = entry['words']\n",
    "        features = torch.tensor(entry['features'], dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        lengths = torch.tensor([len(f) for f in features])\n",
    "\n",
    "        # Perform inference\n",
    "        with torch.no_grad():\n",
    "            output = model(features,lengths)\n",
    "            if num_classes > 2:\n",
    "                print(f'Here and num_classes is {num_classes}')\n",
    "                # Apply softmax to get probabilities for multi-class classification\n",
    "                preds = F.softmax(output.squeeze(0), dim=-1).cpu().numpy()\n",
    "\n",
    "                print(f'preds: {preds}')\n",
    "                \n",
    "                preds = preds.argmax(axis=-1)\n",
    "            else:\n",
    "                # Apply sigmoid for binary classification\n",
    "                preds = (output.squeeze(0) > 0.4).cpu().numpy()\n",
    "\n",
    "        # Map predictions to words\n",
    "        word_predictions = {\n",
    "            word: int(pred) if num_classes > 2 else int(pred[0])\n",
    "            for word, pred in zip(words, preds)\n",
    "        }\n",
    "        results[key] = word_predictions\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions(json_path, predictions, output_path, transcript):\n",
    "    \"\"\"\n",
    "    Save the predictions into a JSON file.\n",
    "\n",
    "    Args:\n",
    "        json_path (str): Path to the original JSON file.\n",
    "        predictions (dict): Dictionary of predictions.\n",
    "        output_path (str): Path to save the new JSON with predictions.\n",
    "        transcript (str): The transcription of the text.\n",
    "    \"\"\"\n",
    "    # Load original data to preserve structure\n",
    "    with open(json_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Prepare the final output with filename, transcript, words, and predictions\n",
    "    final_output = {}\n",
    "\n",
    "    for key, entry in data.items():\n",
    "        final_output[key] = {\n",
    "            'filename': key+\"wav\",  # Assuming the key is the filename\n",
    "            'transcription': transcript,\n",
    "            'words': entry['words'],\n",
    "            'labels': predictions[key]\n",
    "        }\n",
    "\n",
    "    # Save updated data\n",
    "    with open(output_path, 'w') as file:\n",
    "        json.dump(final_output, file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio_to_prosody_labels(audio_file_path, model_path, device='cpu'):\n",
    "    \"\"\"\n",
    "    Processes an audio file to return text transcript and prosody labels.\n",
    "\n",
    "    Args:\n",
    "        audio_file_path (str): Path to the input audio file.\n",
    "        model_path (str): Path to the trained model for prosody label prediction.\n",
    "        device (str): Device to perform inference on (CPU or GPU).\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the text transcript and a dictionary of prosody labels.\n",
    "    \"\"\"\n",
    "    # Use the current working directory as the base directory\n",
    "    current_dir = os.getcwd()\n",
    "\n",
    "    # Create a temporary directory inside the current working directory\n",
    "    temp_dir = os.path.join(current_dir, 'temp')\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        # Step 1: Convert to WAV format if needed\n",
    "        if not audio_file_path.endswith('.wav'):\n",
    "            wav_path = os.path.join(temp_dir, os.path.basename(audio_file_path).replace('.m4a', '.wav'))\n",
    "            convert_to_wav(audio_file_path, wav_path)\n",
    "        else:\n",
    "            wav_path = audio_file_path\n",
    "\n",
    "        # Step 2: Strip silence from the audio\n",
    "        stripped_audio_path = os.path.join(temp_dir, 'stripped_' + os.path.basename(wav_path))\n",
    "        strip_silence_from_audio(wav_path, stripped_audio_path)\n",
    "\n",
    "        # Step 3: Transcribe audio using ASR pipeline\n",
    "        transcript = asr_infer_pipeline(stripped_audio_path).strip()\n",
    "\n",
    "        # Step 4: Generate TextGrid using Aeneas\n",
    "        textgrid_path = call_aeneas_dynamic(stripped_audio_path, transcript, temp_dir)\n",
    "\n",
    "        # Step 5: Create audio slices from TextGrid\n",
    "        audio_slices_dir = os.path.join(temp_dir, 'audio_slices')\n",
    "        os.makedirs(audio_slices_dir, exist_ok=True)\n",
    "        create_audio_slices(stripped_audio_path, textgrid_path, audio_slices_dir)\n",
    "\n",
    "        # Step 6: Extract prosodic features and prepare JSON\n",
    "        features_json_path = os.path.join(temp_dir, 'features.json')\n",
    "        prepare_features_json(audio_slices_dir, features_json_path)\n",
    "\n",
    "        # Step 7: Load the trained model\n",
    "        with open(features_json_path, 'r') as file:\n",
    "            sample_data = json.load(file)\n",
    "        feature_dim = len(next(iter(sample_data.values()))['features'][0])\n",
    "\n",
    "        model = load_model(model_path, feature_dim, device=device)\n",
    "\n",
    "        # Step 8: Predict prosody labels\n",
    "        predictions = predict_labels(features_json_path, model, device, num_classes=4)\n",
    "\n",
    "    finally:\n",
    "        # Clean up the temporary directory\n",
    "        shutil.rmtree(temp_dir)\n",
    "        # pass\n",
    "\n",
    "    return transcript, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Saved stripped audio: /Users/kayems/Documents/GitHub/IUDvPP/ltl/temp/stripped_test05.wav\n",
      "Created directory /Users/kayems/Documents/GitHub/IUDvPP/ltl/temp/text_grid_files\n",
      "[INFO] Validating config string (specify --skip-validator to bypass)...\n",
      "[INFO] Validating config string... done\n",
      "[INFO] Creating task...\n",
      "[INFO] Creating task... done\n",
      "[INFO] Executing task...\n",
      "[INFO] Executing task... done\n",
      "[INFO] Creating output sync map file...\n",
      "[INFO] Creating output sync map file... done\n",
      "\u001b[92m[INFO] Created file '/Users/kayems/Documents/GitHub/IUDvPP/ltl/temp/text_grid_files/stripped_test05.TextGrid'\u001b[0m\n",
      "Generated TextGrid for /Users/kayems/Documents/GitHub/IUDvPP/ltl/temp/stripped_test05.wav to /Users/kayems/Documents/GitHub/IUDvPP/ltl/temp/text_grid_files/stripped_test05.TextGrid\n",
      "<pydub.audio_segment.AudioSegment object at 0x32f541610>\n",
      "Saved: /Users/kayems/Documents/GitHub/IUDvPP/ltl/temp/audio_slices/stripped_test05_Fill_0_0.000_0.355.wav\n",
      "Saved: /Users/kayems/Documents/GitHub/IUDvPP/ltl/temp/audio_slices/stripped_test05_the_1_0.310_0.595.wav\n",
      "Saved: /Users/kayems/Documents/GitHub/IUDvPP/ltl/temp/audio_slices/stripped_test05_Zesia_2_0.550_1.035.wav\n",
      "Saved: /Users/kayems/Documents/GitHub/IUDvPP/ltl/temp/audio_slices/stripped_test05_teapot_3_0.990_1.675.wav\n",
      "Saved: /Users/kayems/Documents/GitHub/IUDvPP/ltl/temp/audio_slices/stripped_test05_with_4_1.630_1.795.wav\n",
      "Saved: /Users/kayems/Documents/GitHub/IUDvPP/ltl/temp/audio_slices/stripped_test05_coke_5_1.750_2.075.wav\n",
      "Saved: /Users/kayems/Documents/GitHub/IUDvPP/ltl/temp/audio_slices/stripped_test05_from_6_2.030_2.275.wav\n",
      "Saved: /Users/kayems/Documents/GitHub/IUDvPP/ltl/temp/audio_slices/stripped_test05_the_7_2.230_2.435.wav\n",
      "Saved: /Users/kayems/Documents/GitHub/IUDvPP/ltl/temp/audio_slices/stripped_test05_cabinet._8_2.390_2.955.wav\n",
      "Saved results to /Users/kayems/Documents/GitHub/IUDvPP/ltl/temp/features.json\n",
      "Here and num_classes is 4\n",
      "preds: [[0.46753255 0.18563119 0.17472243 0.17211385]\n",
      " [0.4479209  0.22130808 0.16533642 0.16543463]\n",
      " [0.32560372 0.3822407  0.14930743 0.14284818]\n",
      " [0.40167728 0.29213107 0.15746374 0.14872803]\n",
      " [0.46340242 0.17775804 0.18728457 0.171555  ]\n",
      " [0.2808894  0.2864796  0.29796955 0.13466147]\n",
      " [0.38428158 0.15257345 0.3194132  0.14373171]\n",
      " [0.46287513 0.17113695 0.19332115 0.17266673]\n",
      " [0.25310948 0.33022454 0.27475584 0.14191021]]\n",
      "Transcript: Fill the Zesia teapot with coke from the cabinet.\n",
      "Prosody Labels: {'test05': {'Fill': 0, 'the': 0, 'Zesia': 1, 'teapot': 0, 'with': 0, 'coke': 2, 'from': 0, 'cabinet.': 1}}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Set seed for reproducibility\n",
    "    seed = 42\n",
    "    set_seed(seed)\n",
    "\n",
    "    # Parameters\n",
    "    audio_file_path = '../ltl/test_samples/test05.m4a'\n",
    "    model_path = '../prosody/models/best-model-ambiguous_instructions-prosody_multiclass.pt'\n",
    "\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "    # Process audio and get results\n",
    "    transcript, prosody_labels = process_audio_to_prosody_labels(audio_file_path, model_path, device=device)\n",
    "\n",
    "    # Output results\n",
    "    print(\"Transcript:\", transcript)\n",
    "    print(\"Prosody Labels:\", prosody_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ltlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
