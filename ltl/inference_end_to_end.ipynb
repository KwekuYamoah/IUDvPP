{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydub in /Users/kayems/.pyenv/versions/3.11.8/envs/ltlenv/lib/python3.11/site-packages (0.25.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# adding the directory containing asr/asr_pipeline.py to the Python path\n",
    "sys.path.append(os.path.abspath(\"../asr\"))\n",
    "#add prosody path to the python path\n",
    "sys.path.append(os.path.abspath(\"../prosody/utils\"))\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../prosody/training_scripts\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence, detect_silence\n",
    "from asr_pipeline import asr_infer_pipeline\n",
    "from prosody_feature_extraction import extract_prosody_features\n",
    "from prosody_bilstm_features_only import Encoder, Decoder, Seq2Seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASR\n",
    "Section to load audio from disk and get transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_wav(input_file: str, output_file: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts an audio file to .wav format.\n",
    "\n",
    "    Args:\n",
    "        input_file (str): The path to the input audio file.\n",
    "        output_file (str): The path to the output .wav file.\n",
    "\n",
    "    Returns:\n",
    "        str: The path to the converted .wav file.\n",
    "    \"\"\"\n",
    "    # Load the audio file\n",
    "    audio = AudioSegment.from_file(input_file)\n",
    "\n",
    "    # Export as .wav\n",
    "    audio.export(output_file, format=\"wav\")\n",
    "\n",
    "    return output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_silence_from_audio(audio_path, output_path, silence_thresh=-40, min_silence_len=500, keep_silence=100):\n",
    "    # Load the audio file\n",
    "    audio = AudioSegment.from_file(audio_path)\n",
    "\n",
    "    # Split audio on silence\n",
    "    chunks = split_on_silence(\n",
    "        audio,\n",
    "        # min_silence_len=min_silence_len,\n",
    "        silence_thresh=silence_thresh,\n",
    "        # keep_silence=keep_silence\n",
    "    )\n",
    "\n",
    "    # Concatenate the chunks back together\n",
    "    stripped_audio = AudioSegment.empty()\n",
    "    for chunk in chunks:\n",
    "        stripped_audio += chunk\n",
    "\n",
    "    # Export the new audio file without silence\n",
    "    # stripped_audio.export(output_path, format=\"wav\")\n",
    "    audio.export(output_path, format=\"wav\")\n",
    "    print(f\"Saved stripped audio: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covert audio file to wav format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create TextGrid File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_aeneas_dynamic(audio_file_path: str, text_transcript: str, temp_dir: str) -> str:\n",
    "    \"\"\"\n",
    "    Call Aeneas to generate a TextGrid file for the given audio file and transcript.\n",
    "\n",
    "    Args:\n",
    "        audio_file_path (str): The path to the audio file.\n",
    "        text_transcript (str): The transcription text.\n",
    "        temp_dir (str): Directory for storing temporary files.\n",
    "\n",
    "    Returns:\n",
    "        str: The relative path to the generated TextGrid file.\n",
    "\n",
    "    Raises:\n",
    "        subprocess.CalledProcessError: If the command to run Aeneas fails.\n",
    "    \"\"\"\n",
    "    # Define directories based on the current working directory\n",
    "    current_dir = os.getcwd()\n",
    "    PROJECT_DIR = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "    AENEAS_DIR = os.path.join(PROJECT_DIR, 'aeneas')\n",
    "    TEMP_TEXT_FILE_PATH = os.path.join(temp_dir, 'temp_transcription.txt')\n",
    "    TEXT_GRID_DIR = os.path.join(temp_dir, 'text_grid_files')  # Create a text_grid_files folder in the temp directory\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    if not os.path.exists(TEXT_GRID_DIR):\n",
    "        os.makedirs(TEXT_GRID_DIR)\n",
    "        print(f\"Created directory {TEXT_GRID_DIR}\")\n",
    "\n",
    "    # Write the transcription text to the temp file\n",
    "    with open(TEMP_TEXT_FILE_PATH, 'w', encoding='utf-8') as temp_text_file:\n",
    "        for word in text_transcript.split():\n",
    "            temp_text_file.write(word + '\\n')\n",
    "\n",
    "    # Define the output file path\n",
    "    output_file_path = os.path.join(TEXT_GRID_DIR, f\"{os.path.splitext(os.path.basename(audio_file_path))[0]}.TextGrid\")\n",
    "\n",
    "    try:\n",
    "        # Change to the aeneas directory\n",
    "        os.chdir(AENEAS_DIR)\n",
    "\n",
    "        # Define the command to run aeneas\n",
    "        command = [\n",
    "            'python3', '-m', 'aeneas.tools.execute_task',\n",
    "            audio_file_path,\n",
    "            TEMP_TEXT_FILE_PATH,\n",
    "            'task_language=eng|is_text_type=plain|os_task_file_format=aud',\n",
    "            output_file_path\n",
    "        ]\n",
    "\n",
    "        # Run the command\n",
    "        subprocess.run(command, check=True)\n",
    "        print(f\"Generated TextGrid for {audio_file_path} to {output_file_path}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Failed to generate TextGrid for {audio_file_path}: {e}\")\n",
    "    finally:\n",
    "        # Return to the original directory\n",
    "        os.chdir(current_dir)\n",
    "        # Clean up the temporary text file\n",
    "        if os.path.exists(TEMP_TEXT_FILE_PATH):\n",
    "            os.remove(TEMP_TEXT_FILE_PATH)\n",
    "\n",
    "    return os.path.relpath(output_file_path, start=current_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Audio Slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_audio_slices(audio_path, textgrid_path, output_folder):\n",
    "    # Load the audio file\n",
    "    audio = AudioSegment.from_file(audio_path)\n",
    "    print(audio)\n",
    "\n",
    "    # Load the TextGrid file and read its contents\n",
    "    with open(textgrid_path, 'r') as f:\n",
    "        file_contents = f.readlines()\n",
    "\n",
    "    # Iterate through each line in the TextGrid file\n",
    "    for index, line in enumerate(file_contents):\n",
    "        # Split the line into start time, end time, and word\n",
    "        start_time_str, end_time_str, word = line.strip().split('\\t')\n",
    "        start_time = float(start_time_str)\n",
    "        end_time = float(end_time_str)\n",
    "\n",
    "        # Skip empty words\n",
    "        if not word:\n",
    "            continue\n",
    "\n",
    "        # Create a buffer window around the word, except for the first word\n",
    "        if index != 0:\n",
    "            start_time = max(0, start_time - 0.010)  # Subtract 5 milliseconds\n",
    "        end_time = end_time + 0.035  # Add 35 milliseconds\n",
    "\n",
    "        # Create a slice of the original audio\n",
    "        start_ms = int(start_time * 1000)  # Convert to milliseconds\n",
    "        end_ms = int(end_time * 1000)  # Convert to milliseconds\n",
    "        audio_slice = audio[start_ms:end_ms]\n",
    "\n",
    "        # Construct the filename\n",
    "        filename = f\"{os.path.basename(audio_path).split('.')[0]}_{word}_{index}_{start_time:.3f}_{end_time:.3f}.wav\"\n",
    "        output_path = os.path.join(output_folder, filename)\n",
    "\n",
    "        # Export the slice\n",
    "        audio_slice.export(output_path, format=\"wav\")\n",
    "        print(f\"Saved: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Json Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features_json(input_folder, output_json_path):\n",
    "    \"\"\"\n",
    "    Processes audio slices to extract prosodic features and save them in a JSON file.\n",
    "\n",
    "    Args:\n",
    "        input_folder (str): Folder containing sliced audio files.\n",
    "        output_json_path (str): Path to save the JSON file.\n",
    "    \"\"\"\n",
    "    # Create a dictionary to store the results\n",
    "    results = {}\n",
    "\n",
    "    # Iterate through each file in the input folder\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(\".wav\"):\n",
    "            # Construct the full file path\n",
    "            audio_path = os.path.join(input_folder, filename)\n",
    "\n",
    "            # Extract the root name, word, and position from the filename\n",
    "            parts = filename.split('_')\n",
    "            #remove the first index in parts\n",
    "            parts.pop(0)\n",
    "            print(parts)\n",
    "\n",
    "            # change to reflect the new naming convention\n",
    "            \n",
    "            # root_name = parts[0]\n",
    "            # word = parts[1]\n",
    "            # position = int(parts[2])\n",
    "            # start_time = float(parts[-2])\n",
    "            # end_time = float(parts[-1].replace('.wav', ''))\n",
    "\n",
    "            root_name = '_'.join(parts[0:4]) # get root name from index 0-3\n",
    "            word = parts[4].rstrip('.')  # Remove full stop if it exists\n",
    "            position = int(parts[5])\n",
    "            start_time = float(parts[6])\n",
    "            end_time = float(parts[7].replace('.wav', ''))\n",
    "\n",
    "            # Extract prosodic features\n",
    "            features = extract_prosody_features(audio_path)\n",
    "            features[torch.isnan(features)] = 0\n",
    "\n",
    "            # Convert the tensor to a list\n",
    "            features_list = features.tolist()\n",
    "\n",
    "            # get current directory\n",
    "            current_dir = os.getcwd()\n",
    "\n",
    "            # Prepare data structure for the file\n",
    "            if root_name not in results:\n",
    "                results[root_name] = {\n",
    "                    \"filepath\": os.path.join(current_dir, root_name + \".wav\"),\n",
    "                    \"words\": [],\n",
    "                    \"positions\": [],\n",
    "                    \"features\": []\n",
    "                }\n",
    "\n",
    "            # Append the word, position, and features to the respective lists\n",
    "            results[root_name][\"words\"].append((position, word))\n",
    "            results[root_name][\"positions\"].append((position, position))\n",
    "            results[root_name][\"features\"].append((position, features_list))\n",
    "\n",
    "    # Sort the words, positions, and features by their positions\n",
    "    for root_name, data in results.items():\n",
    "        data[\"words\"] = [word.lower() for position, word in sorted(data[\"words\"])]\n",
    "        data[\"positions\"] = [position for position, _ in sorted(data[\"positions\"])]\n",
    "        data[\"features\"] = [features for position, features in sorted(data[\"features\"])]\n",
    "\n",
    "    # Write the results to a JSON file\n",
    "    with open(output_json_path, 'w') as json_file:\n",
    "        json.dump(results, json_file, indent=4)\n",
    "    print(f\"Saved results to {output_json_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer with Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path, feature_dim, hidden_dim=128, output_dim=4, num_layers=2, dropout=0.418213662555253, num_attention_layers=2, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Load the trained model from a specified file.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): Path to the model file.\n",
    "        feature_dim (int): Dimension of the input features.\n",
    "        hidden_dim (int): Dimension of the hidden state in LSTM.\n",
    "        output_dim (int): Dimension of the output layer.\n",
    "        num_layers (int): Number of LSTM layers.\n",
    "        dropout (float): Dropout rate.\n",
    "        num_attention_layers (int): Number of attention layers.\n",
    "        device (torch.device): Device to load the model onto (CPU or GPU).\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: Loaded Seq2Seq model.\n",
    "    \"\"\"\n",
    "    encoder = Encoder(feature_dim, hidden_dim, num_layers, dropout).to(device)\n",
    "    decoder = Decoder(hidden_dim, output_dim, num_layers, dropout, num_attention_layers).to(device)\n",
    "    model = Seq2Seq(encoder, decoder).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_labels(json_path, model, device, num_classes=2):\n",
    "    \"\"\"\n",
    "    Perform inference on a JSON file to predict labels for words.\n",
    "\n",
    "    Args:\n",
    "        json_path (str): Path to the input JSON file.\n",
    "        model (nn.Module): Trained model for prediction.\n",
    "        device (torch.device): Device to perform inference on (CPU or GPU).\n",
    "        num_classes (int): Number of classes in the classification task.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with word predictions.\n",
    "    \"\"\"\n",
    "    # Load data from JSON\n",
    "    with open(json_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    results = {}\n",
    "    for key, entry in data.items():\n",
    "        words = entry['words']\n",
    "        features = torch.tensor(entry['features'], dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        lengths = torch.tensor([len(f) for f in features])\n",
    "\n",
    "        # Perform inference\n",
    "        with torch.no_grad():\n",
    "            output = model(features,lengths)\n",
    "            #print model output\n",
    "            print(f'output: {output}')\n",
    "            if num_classes > 2:\n",
    "                print(f'Here and num_classes is {num_classes}')\n",
    "                # Apply softmax to get probabilities for multi-class classification\n",
    "                preds = F.softmax(output.squeeze(0), dim=-1).cpu().numpy()\n",
    "\n",
    "                print(f'preds: {preds}')\n",
    "                \n",
    "                preds = preds.argmax(axis=-1)\n",
    "            else:\n",
    "                # Apply sigmoid for binary classification\n",
    "                preds = (output.squeeze(0) > 0.4).cpu().numpy()\n",
    "\n",
    "        # Map predictions to words\n",
    "        word_predictions = {\n",
    "            word: int(pred) if num_classes > 2 else int(pred[0])\n",
    "            for word, pred in zip(words, preds)\n",
    "        }\n",
    "        results[key] = word_predictions\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions(json_path, predictions, output_path, transcript):\n",
    "    \"\"\"\n",
    "    Save the predictions into a JSON file.\n",
    "\n",
    "    Args:\n",
    "        json_path (str): Path to the original JSON file.\n",
    "        predictions (dict): Dictionary of predictions.\n",
    "        output_path (str): Path to save the new JSON with predictions.\n",
    "        transcript (str): The transcription of the text.\n",
    "    \"\"\"\n",
    "    # Load original data to preserve structure\n",
    "    with open(json_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Prepare the final output with filename, transcript, words, and predictions\n",
    "    final_output = {}\n",
    "\n",
    "    for key, entry in data.items():\n",
    "        final_output[key] = {\n",
    "            'filename': key+\"wav\",  # Assuming the key is the filename\n",
    "            'transcription': transcript,\n",
    "            'words': entry['words'],\n",
    "            'labels': predictions[key]\n",
    "        }\n",
    "\n",
    "    # Save updated data\n",
    "    with open(output_path, 'w') as file:\n",
    "        json.dump(final_output, file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio_to_prosody_labels(audio_file_path, model_path, device='cpu'):\n",
    "    \"\"\"\n",
    "    Processes an audio file to return text transcript and prosody labels.\n",
    "\n",
    "    Args:\n",
    "        audio_file_path (str): Path to the input audio file.\n",
    "        model_path (str): Path to the trained model for prosody label prediction.\n",
    "        device (str): Device to perform inference on (CPU or GPU).\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the text transcript and a dictionary of prosody labels.\n",
    "    \"\"\"\n",
    "    # Use the current working directory as the base directory\n",
    "    current_dir = os.getcwd()\n",
    "\n",
    "    # Create a temporary directory inside the current working directory\n",
    "    temp_dir = os.path.join(current_dir, 'temp')\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        # Step 1: Convert to WAV format if needed\n",
    "        if not audio_file_path.endswith('.wav'):\n",
    "            wav_path = os.path.join(temp_dir, os.path.basename(audio_file_path).replace('.m4a', '.wav'))\n",
    "            convert_to_wav(audio_file_path, wav_path)\n",
    "        else:\n",
    "            wav_path = audio_file_path\n",
    "\n",
    "        # Step 2: Strip silence from the audio\n",
    "        stripped_audio_path = os.path.join(temp_dir, 'stripped_' + os.path.basename(wav_path))\n",
    "        strip_silence_from_audio(wav_path, stripped_audio_path)\n",
    "\n",
    "        # Step 3: Transcribe audio using ASR pipeline\n",
    "        transcript = asr_infer_pipeline(stripped_audio_path).strip()\n",
    "\n",
    "        # Step 4: Generate TextGrid using Aeneas\n",
    "        textgrid_path = call_aeneas_dynamic(stripped_audio_path, transcript, temp_dir)\n",
    "\n",
    "        # Step 5: Create audio slices from TextGrid\n",
    "        audio_slices_dir = os.path.join(temp_dir, 'audio_slices')\n",
    "        os.makedirs(audio_slices_dir, exist_ok=True)\n",
    "        create_audio_slices(stripped_audio_path, textgrid_path, audio_slices_dir)\n",
    "\n",
    "        # Step 6: Extract prosodic features and prepare JSON\n",
    "        features_json_path = os.path.join(temp_dir, 'features.json')\n",
    "        prepare_features_json(audio_slices_dir, features_json_path)\n",
    "\n",
    "        # Step 7: Load the trained model\n",
    "        with open(features_json_path, 'r') as file:\n",
    "            sample_data = json.load(file)\n",
    "        feature_dim = len(next(iter(sample_data.values()))['features'][0])\n",
    "\n",
    "        model = load_model(model_path, feature_dim, device=device)\n",
    "\n",
    "        # Step 8: Predict prosody labels\n",
    "        predictions = predict_labels(features_json_path, model, device, num_classes=4)\n",
    "\n",
    "    finally:\n",
    "        # Clean up the temporary directory\n",
    "        shutil.rmtree(temp_dir)\n",
    "        # pass\n",
    "\n",
    "    return transcript, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Saved stripped audio: /Users/kayems/Documents/GitHub/IUDvPP/ltl/temp/stripped_pid_f01_017_i1.wav\n",
      "New TS: place the coke can beside the pringles on the counter.\n",
      "Created directory /Users/kayems/Documents/GitHub/IUDvPP/ltl/temp/text_grid_files\n",
      "[INFO] Validating config string (specify --skip-validator to bypass)...\n",
      "[INFO] Validating config string... done\n",
      "[INFO] Creating task...\n",
      "[INFO] Creating task... done\n",
      "[INFO] Executing task...\n",
      "[INFO] Executing task... done\n",
      "[INFO] Creating output sync map file...\n",
      "[INFO] Creating output sync map file... done\n",
      "\u001b[92m[INFO] Created file '/Users/kayems/Documents/GitHub/IUDvPP/ltl/temp/text_grid_files/stripped_pid_f01_017_i1.TextGrid'\u001b[0m\n",
      "Generated TextGrid for /Users/kayems/Documents/GitHub/IUDvPP/ltl/temp/stripped_pid_f01_017_i1.wav to /Users/kayems/Documents/GitHub/IUDvPP/ltl/temp/text_grid_files/stripped_pid_f01_017_i1.TextGrid\n",
      "<pydub.audio_segment.AudioSegment object at 0x32c90e850>\n",
      "Saved: /Users/kayems/Documents/GitHub/IUDvPP/ltl/temp/audio_slices/stripped_pid_f01_017_i1_Place_0_0.000_0.795.wav\n",
      "Saved: /Users/kayems/Documents/GitHub/IUDvPP/ltl/temp/audio_slices/stripped_pid_f01_017_i1_the_1_0.750_1.035.wav\n",
      "Saved: /Users/kayems/Documents/GitHub/IUDvPP/ltl/temp/audio_slices/stripped_pid_f01_017_i1_Coke_2_0.990_1.275.wav\n",
      "Saved: /Users/kayems/Documents/GitHub/IUDvPP/ltl/temp/audio_slices/stripped_pid_f01_017_i1_can_3_1.230_1.595.wav\n",
      "Saved: /Users/kayems/Documents/GitHub/IUDvPP/ltl/temp/audio_slices/stripped_pid_f01_017_i1_beside_4_1.550_2.075.wav\n",
      "Saved: /Users/kayems/Documents/GitHub/IUDvPP/ltl/temp/audio_slices/stripped_pid_f01_017_i1_the_5_2.030_2.075.wav\n",
      "Saved: /Users/kayems/Documents/GitHub/IUDvPP/ltl/temp/audio_slices/stripped_pid_f01_017_i1_Pringles_6_2.030_3.195.wav\n",
      "Saved: /Users/kayems/Documents/GitHub/IUDvPP/ltl/temp/audio_slices/stripped_pid_f01_017_i1_on_7_3.150_3.755.wav\n",
      "Saved: /Users/kayems/Documents/GitHub/IUDvPP/ltl/temp/audio_slices/stripped_pid_f01_017_i1_the_8_3.710_4.035.wav\n",
      "Saved: /Users/kayems/Documents/GitHub/IUDvPP/ltl/temp/audio_slices/stripped_pid_f01_017_i1_counter._9_3.990_4.275.wav\n",
      "['pid', 'f01', '017', 'i1', 'counter.', '9', '3.990', '4.275.wav']\n",
      "['pid', 'f01', '017', 'i1', 'can', '3', '1.230', '1.595.wav']\n",
      "['pid', 'f01', '017', 'i1', 'Coke', '2', '0.990', '1.275.wav']\n",
      "['pid', 'f01', '017', 'i1', 'Pringles', '6', '2.030', '3.195.wav']\n",
      "['pid', 'f01', '017', 'i1', 'beside', '4', '1.550', '2.075.wav']\n",
      "['pid', 'f01', '017', 'i1', 'on', '7', '3.150', '3.755.wav']\n",
      "['pid', 'f01', '017', 'i1', 'the', '5', '2.030', '2.075.wav']\n",
      "['pid', 'f01', '017', 'i1', 'Place', '0', '0.000', '0.795.wav']\n",
      "['pid', 'f01', '017', 'i1', 'the', '1', '0.750', '1.035.wav']\n",
      "['pid', 'f01', '017', 'i1', 'the', '8', '3.710', '4.035.wav']\n",
      "Saved results to /Users/kayems/Documents/GitHub/IUDvPP/ltl/temp/features.json\n",
      "output: tensor([[[9.9997e-01, 1.6198e-02, 6.7161e-03, 5.8443e-04],\n",
      "         [9.9979e-01, 3.1358e-01, 2.4796e-03, 3.3527e-03],\n",
      "         [9.0488e-01, 9.8349e-01, 3.2557e-02, 4.1661e-04],\n",
      "         [9.9330e-01, 8.1804e-01, 3.7329e-02, 1.5732e-04],\n",
      "         [9.9802e-01, 1.7641e-01, 6.5941e-02, 1.7284e-03],\n",
      "         [8.8193e-01, 6.2076e-01, 6.1949e-01, 7.1312e-04],\n",
      "         [9.4177e-01, 4.0284e-01, 7.9004e-01, 7.7711e-04],\n",
      "         [9.9972e-01, 1.4574e-02, 2.1647e-01, 4.8937e-04],\n",
      "         [9.0143e-01, 6.9761e-01, 6.2161e-01, 3.6845e-04],\n",
      "         [5.1132e-01, 6.7574e-01, 8.1969e-01, 9.8166e-04]]])\n",
      "Here and num_classes is 4\n",
      "preds: [[0.47340176 0.17700364 0.17533319 0.17426138]\n",
      " [0.44612157 0.22461279 0.1645609  0.16470464]\n",
      " [0.34429047 0.37244776 0.14390674 0.139355  ]\n",
      " [0.3854924  0.32352    0.14819762 0.14278999]\n",
      " [0.4539883  0.19962886 0.17875008 0.16763277]\n",
      " [0.3385682  0.26075077 0.26041877 0.14026222]\n",
      " [0.35300395 0.20593181 0.30330828 0.13775595]\n",
      " [0.45486206 0.16983907 0.20783505 0.16746378]\n",
      " [0.33583274 0.27390984 0.2538625  0.13639498]\n",
      " [0.2415347  0.2846984  0.3287759  0.14499104]]\n",
      "Transcript: Place the Coke can beside the Pringles on the counter.\n",
      "Prosody Labels: {'pid_f01_017_i1': {'place': 0, 'the': 0, 'coke': 1, 'can': 0, 'beside': 0, 'pringles': 0, 'on': 0, 'counter': 2}}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Set seed for reproducibility\n",
    "    seed = 42\n",
    "    set_seed(seed)\n",
    "\n",
    "    # Parameters\n",
    "    audio_file_path = '../voice_sample_two_ways/pid_f01/pid_f01_017_i1.wav'\n",
    "    model_path = '../prosody/models/best-model-ambiguous_instructions-prosody_multiclass.pt'\n",
    "\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "    # Process audio and get results\n",
    "    transcript, prosody_labels = process_audio_to_prosody_labels(audio_file_path, model_path, device=device)\n",
    "\n",
    "    # Output results\n",
    "    print(\"Transcript:\", transcript)\n",
    "    print(\"Prosody Labels:\", prosody_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ltlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
